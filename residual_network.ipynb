{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import time\n",
    "from model.residual_attention_network import ResidualAttentionModel_92_32input_update as ResidualAttentionModel\n",
    "# 正确率0.954模型\n",
    "model_file = 'model_92_sgd.pkl'\n",
    "batch_size = 16\n",
    "class_num = 10\n",
    "\n",
    "# cifar-10: Acc-95.4(Top-1 err 4.6) with ResidualAttentionModel_92_32input_update(higher than paper top-1 err 4.99)\n",
    "# cifar-10: Acc-96.65(Top-1 err 3.35) with ResidualAttentionModel_92_32input_update(with mixup).\n",
    "# cifar-10: Acc-96.84(Top-1 err 3.16) with ResidualAttentionModel_92_32input_update(with mixup, with simpler attention module).\n",
    "\n",
    "\n",
    "# residualblock\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.stride = stride\n",
    "        self.bn1 = nn.BatchNorm2d(input_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(input_channels, output_channels//4, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(output_channels//4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(output_channels//4, output_channels//4, 3, stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(output_channels//4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(output_channels//4, output_channels, 1, 1, bias=False)\n",
    "        # 相加时保持维度一致\n",
    "        self.conv4 = nn.Conv2d(input_channels, output_channels, 1, stride, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.bn1(x)\n",
    "        out1 = self.relu(out)\n",
    "        out = self.conv1(out1)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        # 调整维度和大小一致才能相加\n",
    "        if (self.input_channels != self.output_channels) or (self.stride !=1):\n",
    "            residual = self.conv4(out1)\n",
    "        out += residual\n",
    "        return out\n",
    "\n",
    "    \n",
    "# stage\n",
    "# 为448*448图片服务的stage，4层\n",
    "class AttentionModule_stage0(nn.Module):\n",
    "    # input size is 112*112\n",
    "    def __init__(self, in_channels, out_channels, size1=(112, 112), size2=(56, 56), size3=(28, 28), size4=(14, 14)):\n",
    "        super(AttentionModule_stage0, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "\n",
    "        # 第一层\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # 56*56\n",
    "        self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        # 第二层\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # 28*28\n",
    "        self.softmax2_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.skip2_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        # 第三层\n",
    "        self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # 14*14\n",
    "        self.softmax3_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.skip3_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        # 第四层\n",
    "        self.mpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # 7*7\n",
    "        self.softmax4_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "        self.interpolation4 = nn.UpsamplingBilinear2d(size=size4)\n",
    "\n",
    "        # 第三层\n",
    "        self.softmax5_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation3 = nn.UpsamplingBilinear2d(size=size3)\n",
    "\n",
    "        # 第二层\n",
    "        self.softmax6_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "\n",
    "        # 第一层\n",
    "        self.softmax7_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "        self.softmax8_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels , kernel_size=1, stride=1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 112*112\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        # 56*56\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "        out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "        out_mpool2 = self.mpool2(out_softmax1)\n",
    "        # 28*28\n",
    "        out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "        out_skip2_connection = self.skip2_connection_residual_block(out_softmax2)\n",
    "        out_mpool3 = self.mpool3(out_softmax2)\n",
    "        # 14*14\n",
    "        out_softmax3 = self.softmax3_blocks(out_mpool3)\n",
    "        out_skip3_connection = self.skip3_connection_residual_block(out_softmax3)\n",
    "        out_mpool4 = self.mpool4(out_softmax3)\n",
    "        # 7*7\n",
    "        out_softmax4 = self.softmax4_blocks(out_mpool4)\n",
    "        out_interp4 = self.interpolation4(out_softmax4) + out_softmax3\n",
    "        out = out_interp4 + out_skip3_connection\n",
    "        out_softmax5 = self.softmax5_blocks(out)\n",
    "        out_interp3 = self.interpolation3(out_softmax5) + out_softmax2\n",
    "        # print(out_skip2_connection.data)\n",
    "        # print(out_interp3.data)\n",
    "        out = out_interp3 + out_skip2_connection\n",
    "        out_softmax6 = self.softmax6_blocks(out)\n",
    "        out_interp2 = self.interpolation2(out_softmax6) + out_softmax1\n",
    "        out = out_interp2 + out_skip1_connection\n",
    "        out_softmax7 = self.softmax7_blocks(out)\n",
    "        out_interp1 = self.interpolation1(out_softmax7) + out_trunk\n",
    "        out_softmax8 = self.softmax8_blocks(out_interp1)\n",
    "        out = (1 + out_softmax8) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "\n",
    "        return out_last\n",
    "\n",
    "\n",
    "# 为224服务的stage，3层\n",
    "class AttentionModule_stage1(nn.Module):\n",
    "    # input size is 56*56\n",
    "    def __init__(self, in_channels, out_channels, size1=(56, 56), size2=(28, 28), size3=(14, 14)):\n",
    "        super(AttentionModule_stage1, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.softmax2_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.skip2_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.softmax3_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "        self.interpolation3 = nn.UpsamplingBilinear2d(size=size3)\n",
    "\n",
    "        self.softmax4_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "\n",
    "        self.softmax5_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "        # stage1的最后两次1*1卷积和sigmoid函数（mixed attention）\n",
    "        self.softmax6_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        # 主干分支\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        # 第一层\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "        out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "        # 第二层\n",
    "        out_mpool2 = self.mpool2(out_softmax1)\n",
    "        out_softmax2 = self.softmax2_blocks(out_mpool2) # 14*14\n",
    "        out_skip2_connection = self.skip2_connection_residual_block(out_softmax2)\n",
    "        # 第三层\n",
    "        out_mpool3 = self.mpool3(out_softmax2)\n",
    "        out_softmax3 = self.softmax3_blocks(out_mpool3)# 7*7\n",
    "        out_interp3 = self.interpolation3(out_softmax3) + out_softmax2\n",
    "        out = out_interp3 + out_skip2_connection\n",
    "        # 第二层\n",
    "        out_softmax4 = self.softmax4_blocks(out)\n",
    "        out_interp2 = self.interpolation2(out_softmax4) + out_softmax1\n",
    "        out = out_interp2 + out_skip1_connection\n",
    "        # 第一层\n",
    "        out_softmax5 = self.softmax5_blocks(out)\n",
    "        out_interp1 = self.interpolation1(out_softmax5) + out_trunk\n",
    "\n",
    "        out_softmax6 = self.softmax6_blocks(out_interp1)\n",
    "        out = (1 + out_softmax6) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "        return out_last\n",
    "\n",
    "\n",
    "class AttentionModule_stage2(nn.Module):\n",
    "    # input image size is 28*28\n",
    "    def __init__(self, in_channels, out_channels, size1=(28, 28), size2=(14, 14)):\n",
    "        super(AttentionModule_stage2, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.softmax2_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "\n",
    "        self.softmax3_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "        self.softmax4_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "        out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "\n",
    "        out_mpool2 = self.mpool2(out_softmax1)\n",
    "        out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "        out_interp2 = self.interpolation2(out_softmax2) + out_softmax1\n",
    "        out = out_interp2 + out_skip1_connection\n",
    "\n",
    "        out_softmax3 = self.softmax3_blocks(out)\n",
    "        out_interp1 = self.interpolation1(out_softmax3) + out_trunk\n",
    "\n",
    "        out_softmax4 = self.softmax4_blocks(out_interp1)\n",
    "        out = (1 + out_softmax4) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "        return out_last\n",
    "\n",
    "\n",
    "class AttentionModule_stage3(nn.Module):\n",
    "    # input image size is 14*14\n",
    "    def __init__(self, in_channels, out_channels, size1=(14, 14)):\n",
    "        super(AttentionModule_stage3, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.softmax1_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "        self.softmax2_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "        out_interp1 = self.interpolation1(out_softmax1) + out_trunk\n",
    "        out_softmax2 = self.softmax2_blocks(out_interp1)\n",
    "        out = (1 + out_softmax2) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "        return out_last\n",
    "\n",
    "\n",
    "# 适用于cifar的网络\n",
    "class AttentionModule_stage1_cifar(nn.Module):\n",
    "    # 把最开始卷积变成大小不变的卷积，池化也去掉了，所以输入为32*32\n",
    "    def __init__(self, in_channels, out_channels, size1=(32, 32), size2=(16, 16), size3=(8, 8)):\n",
    "        super(AttentionModule_stage1_cifar, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "        # 第一层\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 16*16\n",
    "        self.down_residual_blocks1 = ResidualBlock(in_channels, out_channels)\n",
    "        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "        # 第二层\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 8*8\n",
    "        self.down_residual_blocks2 = ResidualBlock(in_channels, out_channels)\n",
    "        self.skip2_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "        # 第三层\n",
    "        self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 4*4\n",
    "        self.middle_3r_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "        self.interpolation3 = nn.UpsamplingBilinear2d(size=size3)  # 8*8\n",
    "        # 第二层\n",
    "        self.up_residual_blocks2 = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)  # 16*16\n",
    "        # 第一层\n",
    "        self.up_residual_blocks1 = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)  # 32*32\n",
    "\n",
    "        self.conv1_1_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        # 第一层\n",
    "        out_mpool1 = self.mpool1(x)  # 16*16\n",
    "        out_down_residual_blocks1 = self.down_residual_blocks1(out_mpool1)\n",
    "        out_skip1_connection = self.skip1_connection_residual_block(out_down_residual_blocks1)\n",
    "        # 第二层\n",
    "        out_mpool2 = self.mpool2(out_down_residual_blocks1)  # 8*8\n",
    "        out_down_residual_blocks2 = self.down_residual_blocks2(out_mpool2)\n",
    "        out_skip2_connection = self.skip2_connection_residual_block(out_down_residual_blocks2)\n",
    "        # 第三层\n",
    "        out_mpool3 = self.mpool3(out_down_residual_blocks2)  # 4*4\n",
    "        out_middle_3r_blocks = self.middle_3r_blocks(out_mpool3)\n",
    "        out_interp3 = self.interpolation3(out_middle_3r_blocks) + out_down_residual_blocks2  # 8*8\n",
    "        out = out_interp3 + out_skip2_connection\n",
    "        # 第二层\n",
    "        out_up_residual_blocks2 = self.up_residual_blocks1(out)  # 8*8\n",
    "        out_interp2 = self.interpolation2(out_up_residual_blocks2) + out_down_residual_blocks1  # 16*16\n",
    "        out = out_interp2 + out_skip1_connection\n",
    "        # 第一层\n",
    "        out_up_residual_blocks3 = self.up_residual_blocks1(out)  # 16*16\n",
    "        out_interp1 = self.interpolation1(out_up_residual_blocks3) + out_trunk  # 32*32\n",
    "        out_conv1_1_blocks = self.conv1_1_blocks(out_interp1)\n",
    "\n",
    "        out = (1 + out_conv1_1_blocks) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "        return out_last\n",
    "\n",
    "\n",
    "class AttentionModule_stage2_cifar(nn.Module):\n",
    "    # 16*16\n",
    "    def __init__(self, in_channels, out_channels, size1=(16, 16), size2=(8, 8)):\n",
    "        super(AttentionModule_stage2_cifar, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "        # 第一层\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 8*8\n",
    "        self.down_residual_blocks1 = ResidualBlock(in_channels, out_channels)\n",
    "        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "        # 第二层\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 4*4\n",
    "        self.middle_2r_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size2)  # 8*8\n",
    "        # 第一层\n",
    "        self.up_residual_blocks1 = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=size1)  # 16*16\n",
    "\n",
    "        self.conv1_1_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        # 第一层\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        out_down_residual_blocks1 = self.down_residual_blocks1(out_mpool1)\n",
    "        out_skip1_connection = self.skip1_connection_residual_block(out_down_residual_blocks1)\n",
    "        # 第二层\n",
    "        out_mpool2 = self.mpool2(out_down_residual_blocks1)\n",
    "        out_middle_2r_blocks = self.middle_2r_blocks(out_mpool2)\n",
    "        out_interp = self.interpolation1(out_middle_2r_blocks) + out_down_residual_blocks1\n",
    "        out = out_interp + out_skip1_connection\n",
    "        # 第一层\n",
    "        out_up_residual_blocks1 = self.up_residual_blocks1(out)\n",
    "        out_interp2 = self.interpolation2(out_up_residual_blocks1) + out_trunk\n",
    "\n",
    "        out_conv1_1_blocks = self.conv1_1_blocks(out_interp2)\n",
    "        out = (1 + out_conv1_1_blocks) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "        return out_last\n",
    "\n",
    "\n",
    "class AttentionModule_stage3_cifar(nn.Module):\n",
    "    # input size is 8*8\n",
    "    def __init__(self, in_channels, out_channels, size=(8, 8)):\n",
    "        super(AttentionModule_stage3_cifar, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 4*4\n",
    "        self.middle_2r_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size)  # 8*8\n",
    "\n",
    "        self.conv1_1_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        out_middle_2r_blocks = self.middle_2r_blocks(out_mpool1)\n",
    "        out_interp = self.interpolation1(out_middle_2r_blocks) + out_trunk\n",
    "        out_conv1_1_blocks = self.conv1_1_blocks(out_interp)\n",
    "        out = (1 + out_conv1_1_blocks) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "        return out_last\n",
    "\n",
    "    \n",
    "# 定义模型\n",
    "class ResidualAttentionModel_448_92input(nn.Module):\n",
    "    # for input size 448\n",
    "    def __init__(self):\n",
    "        super(ResidualAttentionModel_448_92input, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # tbq add\n",
    "        # 112*112\n",
    "        self.residual_block0 = ResidualBlock(64, 128)\n",
    "        self.attention_module0 = AttentionModule_stage0(128, 128)\n",
    "        # tbq add end\n",
    "        self.residual_block1 = ResidualBlock(128, 256, 2)\n",
    "        # 56*56\n",
    "        self.attention_module1 = AttentionModule_stage1(256, 256)\n",
    "        self.residual_block2 = ResidualBlock(256, 512, 2)\n",
    "        self.attention_module2 = AttentionModule_stage2(512, 512)\n",
    "        self.attention_module2_2 = AttentionModule_stage2(512, 512)  # tbq add\n",
    "        self.residual_block3 = ResidualBlock(512, 1024, 2)\n",
    "        self.attention_module3 = AttentionModule_stage3(1024, 1024)\n",
    "        self.attention_module3_2 = AttentionModule_stage3(1024, 1024)  # tbq add\n",
    "        self.attention_module3_3 = AttentionModule_stage3(1024, 1024)  # tbq add\n",
    "        self.residual_block4 = ResidualBlock(1024, 2048, 2)\n",
    "        self.residual_block5 = ResidualBlock(2048, 2048)\n",
    "        self.residual_block6 = ResidualBlock(2048, 2048)\n",
    "        self.mpool2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(2048,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.mpool1(out)\n",
    "        out = self.residual_block0(out)\n",
    "        out = self.attention_module0(out)\n",
    "        # print(out.data)\n",
    "        out = self.residual_block1(out)\n",
    "        out = self.attention_module1(out)\n",
    "        out = self.residual_block2(out)\n",
    "        out = self.attention_module2(out)\n",
    "        out = self.attention_module2_2(out)\n",
    "        out = self.residual_block3(out)\n",
    "        # print(out.data)\n",
    "        out = self.attention_module3(out)\n",
    "        out = self.attention_module3_2(out)\n",
    "        out = self.attention_module3_3(out)\n",
    "        out = self.residual_block4(out)\n",
    "        out = self.residual_block5(out)\n",
    "        out = self.residual_block6(out)\n",
    "        out = self.mpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAttentionModel_92(nn.Module):\n",
    "    # for input size 224\n",
    "    def __init__(self):\n",
    "        super(ResidualAttentionModel_92, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.residual_block1 = ResidualBlock(64, 256)\n",
    "        self.attention_module1 = AttentionModule_stage1(256, 256)\n",
    "        self.residual_block2 = ResidualBlock(256, 512, 2)\n",
    "        self.attention_module2 = AttentionModule_stage2(512, 512)\n",
    "        self.attention_module2_2 = AttentionModule_stage2(512, 512)  # tbq add\n",
    "        self.residual_block3 = ResidualBlock(512, 1024, 2)\n",
    "        self.attention_module3 = AttentionModule_stage3(1024, 1024)\n",
    "        self.attention_module3_2 = AttentionModule_stage3(1024, 1024)  # tbq add\n",
    "        self.attention_module3_3 = AttentionModule_stage3(1024, 1024)  # tbq add\n",
    "        self.residual_block4 = ResidualBlock(1024, 2048, 2)\n",
    "        self.residual_block5 = ResidualBlock(2048, 2048)\n",
    "        self.residual_block6 = ResidualBlock(2048, 2048)\n",
    "        self.mpool2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(2048,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.mpool1(out)\n",
    "        # print(out.data)\n",
    "        out = self.residual_block1(out)\n",
    "        out = self.attention_module1(out)\n",
    "        out = self.residual_block2(out)\n",
    "        out = self.attention_module2(out)\n",
    "        out = self.attention_module2_2(out)\n",
    "        out = self.residual_block3(out)\n",
    "        # print(out.data)\n",
    "        out = self.attention_module3(out)\n",
    "        out = self.attention_module3_2(out)\n",
    "        out = self.attention_module3_3(out)\n",
    "        out = self.residual_block4(out)\n",
    "        out = self.residual_block5(out)\n",
    "        out = self.residual_block6(out)\n",
    "        out = self.mpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualAttentionModel_56(nn.Module):\n",
    "    # for input size 224\n",
    "    def __init__(self):\n",
    "        super(ResidualAttentionModel_56, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.residual_block1 = ResidualBlock(64, 256)\n",
    "        self.attention_module1 = AttentionModule_stage1(256, 256)\n",
    "        self.residual_block2 = ResidualBlock(256, 512, 2)\n",
    "        self.attention_module2 = AttentionModule_stage2(512, 512)\n",
    "        self.residual_block3 = ResidualBlock(512, 1024, 2)\n",
    "        self.attention_module3 = AttentionModule_stage3(1024, 1024)\n",
    "        self.residual_block4 = ResidualBlock(1024, 2048, 2)\n",
    "        self.residual_block5 = ResidualBlock(2048, 2048)\n",
    "        self.residual_block6 = ResidualBlock(2048, 2048)\n",
    "        self.mpool2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(2048,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.mpool1(out)\n",
    "        # print(out.data)\n",
    "        out = self.residual_block1(out)\n",
    "        out = self.attention_module1(out)\n",
    "        out = self.residual_block2(out)\n",
    "        out = self.attention_module2(out)\n",
    "        out = self.residual_block3(out)\n",
    "        # print(out.data)\n",
    "        out = self.attention_module3(out)\n",
    "        out = self.residual_block4(out)\n",
    "        out = self.residual_block5(out)\n",
    "        out = self.residual_block6(out)\n",
    "        out = self.mpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# 适用于cifar10\n",
    "class ResidualAttentionModel_92_32input_update(nn.Module):\n",
    "    # for input size 32\n",
    "    def __init__(self):\n",
    "        super(ResidualAttentionModel_92_32input_update, self).__init__()\n",
    "        # 这个卷积stride为1，image_size不再减半\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )  # 32*32\n",
    "        # self.mpool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # 16*16\n",
    "        self.residual_block1 = ResidualBlock(32, 128)  # 32*32\n",
    "        self.attention_module1 = AttentionModule_stage1_cifar(128, 128, size1=(32, 32), size2=(16, 16), size3=(8, 8))  # 32*32\n",
    "        self.residual_block2 = ResidualBlock(128, 256, 2)  # 16*16\n",
    "        self.attention_module2 = AttentionModule_stage2_cifar(256, 256, size1=(16, 16), size2=(8, 8))  # 16*16\n",
    "        self.attention_module2_2 = AttentionModule_stage2_cifar(256, 256, size1=(16, 16), size2=(8, 8))  # 16*16\n",
    "        self.residual_block3 = ResidualBlock(256, 512, 2)  # 8*8\n",
    "        self.attention_module3 = AttentionModule_stage3_cifar(512, 512)  # 8*8\n",
    "        self.attention_module3_2 = AttentionModule_stage3_cifar(512, 512)  # 8*8\n",
    "        self.attention_module3_3 = AttentionModule_stage3_cifar(512, 512)  # 8*8\n",
    "        self.residual_block4 = ResidualBlock(512, 1024, 2)  # 4*4\n",
    "        self.residual_block5 = ResidualBlock(1024, 1024)  # 4*4\n",
    "        self.residual_block6 = ResidualBlock(1024, 1024)  # 4*4\n",
    "        self.mpool2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=4)\n",
    "        )\n",
    "        # cifar10只有10个类\n",
    "        self.fc = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        # out = self.mpool1(out)\n",
    "        # print(out.data)\n",
    "        out = self.residual_block1(out)\n",
    "        out = self.attention_module1(out)\n",
    "        out = self.residual_block2(out)\n",
    "        out = self.attention_module2(out)\n",
    "        out = self.attention_module2_2(out)\n",
    "        out = self.residual_block3(out)\n",
    "        # print(out.data)\n",
    "        out = self.attention_module3(out)\n",
    "        out = self.attention_module3_2(out)\n",
    "        out = self.attention_module3_3(out)\n",
    "        out = self.residual_block4(out)\n",
    "        out = self.residual_block5(out)\n",
    "        out = self.residual_block6(out)\n",
    "        out = self.mpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "# for test\n",
    "def test(model, test_loader, btrain=False, model_state_dict_file=model_file):\n",
    "    # 在不训练时直接测试\n",
    "    if not btrain:\n",
    "        # 载入初始化数据\n",
    "        model.load_state_dict(torch.load(model_state_dict_file))\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 全0列表,分别存储每一类预测正确数目和预测数目\n",
    "    class_correct = list(0. for i in range(class_num))\n",
    "    class_total = list(0. for i in range(class_num))\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "            # 一维度tensor，boolean tensor\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(batch_size):\n",
    "                # 获取真实类型\n",
    "                label = labels[i]\n",
    "                # 真实类型预测正确数变化\n",
    "                class_correct[label] += c[i]\n",
    "                # 真实类型预测总数变化\n",
    "                class_total[label] += 1\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %%'.format(100 * float(correct.item()) / total))\n",
    "    print('Accuracy of the model on the test images:', (float(correct.item())/total))\n",
    "    for i in range(10):\n",
    "        print('Accuracy of {:5s} : {:.2f} %%' .format(classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# 训练这个网络\n",
    "# 定义transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), # 默认概率0.5\n",
    "    transforms.RandomCrop((32, 32), padding=4),   #left, top, right, bottom,\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# when image is rgb, totensor do the division 255\n",
    "# CIFAR-10 Dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data',\n",
    "                               train=True,\n",
    "                               transform=transform,\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data',\n",
    "                              train=False,\n",
    "                              transform=test_transform)\n",
    "\n",
    "# Data Loader (Input Pipeline) windows can not use multi-threading\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,  # 内存不够只能16\n",
    "                                           shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "model = ResidualAttentionModel().cuda()\n",
    "print(model)\n",
    "\n",
    "lr = 0.1  # 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "is_train = True\n",
    "is_pretrain = False\n",
    "acc_best = 0\n",
    "total_epoch = 300\n",
    "if is_train is True:\n",
    "    if is_pretrain == True:\n",
    "        model.load_state_dict((torch.load(model_file)))\n",
    "    # Training\n",
    "    for epoch in range(total_epoch):\n",
    "        model.train()\n",
    "        tims = time.time()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.cuda()\n",
    "            # print(images.data)\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(\"Epoch [{}/{}], Iter [{}/{}] Loss: {:.4f}\" .format(epoch+1, total_epoch, i+1, len(train_loader), loss.item()))\n",
    "        print('the epoch takes time:',time.time()-tims)\n",
    "        print('evaluate test set:')\n",
    "        # 使用我们训练过的模型对测试集进行测试\n",
    "        acc = test(model, test_loader, btrain=True)\n",
    "        if acc > acc_best:\n",
    "            acc_best = acc\n",
    "            print('current best acc,', acc_best)\n",
    "            torch.save(model.state_dict(), model_file)\n",
    "        # Decaying Learning Rate\n",
    "        if (epoch+1) / total_epoch == 0.3 or (epoch+1) / total_epoch == 0.6 or (epoch+1) / total_epoch == 0.9:\n",
    "            lr /= 10\n",
    "            print('reset learning rate to:', lr)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "                print(param_group['lr'])\n",
    "    # 存储最好的模型数据\n",
    "    torch.save(model.state_dict(), 'last_model_92_sgd.pkl')\n",
    "\n",
    "else:\n",
    "    test(model, test_loader, btrain=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
